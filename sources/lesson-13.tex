\documentclass[a4paper,12pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage[margin=2cm]{geometry}
\usepackage{tikz}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}

\input{commands.tex}

\begin{document}

    \title{Занятие 13. Закон больших чисел.}
    \author{Тигетов Давид}
    \maketitle


    \section{Введение}

    Если случайная величина $X$ имеет конечный первый абсолютный момент ($\expectation{\modulus{X}} < \infty$), то для случайной величины $X$ справедливо
    \textbf{первое неравенство Чебышева}:
    \begin{equation}
        \forall \varepsilon > 0 : \probability{ \modulus{X} \ge \varepsilon } \le \frac{\expectation{\modulus{X}}}{\varepsilon} .
    \end{equation}

    Если случайная величина $X$ имеет конечный второй момент ($\expectation{X^2} < \infty$), то для случайной величины $X$ справедливо \textbf{второе неравенство Чебышева}:
    \begin{equation}
        \forall \varepsilon > 0 : \probability{ X \ge \varepsilon } \le \frac{\expectation{X^2}}{\varepsilon^2} .
    \end{equation}

    Если во втором неравенстве Чебышева в качестве величины $X$ использовать величину $\modulus{X - \expectation{X}}$, то оно преобразуется к виду:
    \begin{equation}
        \forall \varepsilon > 0 : \probability{ \modulus{X - \expectation{X}} \ge \varepsilon } \le \frac{\expectation{\modulus{X - \expectation{X}}^2}}{\varepsilon^2} = \frac{\variance{X}}{\varepsilon^2} ,
    \end{equation}

    Для последовательности $\sequence{X_n}$ случайных величин выполняется \textbf{закон больших чисел}, если последовательность среднеарифметических от
    случайных величин:
    \begin{equation}
        S_n = \frac{1}{n} \sum_{k=1}^n X_k
    \end{equation}
    сходится по вероятности к последовательности среднеарифметических от математических ожиданий:
    \begin{equation}
        M_n = \frac{1}{n} \sum_{k=1}^n \expectation{X_k} .
    \end{equation}
    В краткой форме записывают:
    \begin{equation}
        S_n \stackrel{P}{\longrightarrow} M_n
    \end{equation}
    формально сходимость обозначает:
    \begin{equation}
        \forall \varepsilon > 0: \lim_{n \rightarrow \infty} \probability{ \modulus{ S_n - M_n } \ge \varepsilon } = 0 .
    \end{equation}

    Существует целый ряд теорем о том, когда для последовательности $\sequence{X_n}$ выполняется закон больших чисел.
    Одной из таких теорем является \textbf{теорема Чебышева}, которая легко доказывается с помощью неравенства Чебышева:
    если случайные величины $X_n$ попарно независимы и сумма возрастает не быстрее, чем $n^2$:
    \begin{equation}
        \lim_{n \rightarrow \infty} \frac{1}{n^2} \sum_{k=1}^n \variance{X_k} = 0 ,
    \end{equation}
    тогда для последовательности $\sequence{X_n}$ выполняется закон больших чисел.


    \section{Пример 1}
    \subsection*{Условие}
    Автобусы следуют по маршруту со случайным интервалом $X$, имеющим среднее значение 4 минуты. Требуется:
    \begin{enumerate}
        \item оценить сверху вероятность ожидания автобуса не менее 8 минут: $\probability{X \ge 8}$ ,
        \item оценить снизу вероятность ожидания автобуса менее 6 минут: $\probability{X < 6}$.
    \end{enumerate}

    \subsection*{Решение}
    Легко видеть, что величина $X$ является неотрицательной и $\modulus{X} = X$. Отсюда следует конечность абсолютного момента:
    \begin{equation}
        \expectation{\modulus{X}} = \expectation{X} = 4 < \infty ,
    \end{equation}
    поэтому для величины $X$ выполняется первое неравенство Чебышева:
    \begin{equation}
        \forall \varepsilon > 0 : \probability{X \ge \varepsilon} \le \frac{\expectation{X}}{\varepsilon} .
    \end{equation}

    Возьмем в качестве $\varepsilon$ число 8, тогда:
    \begin{equation}
        \probability{X \ge 8} \le \frac{\expectation{X}}{8} = \frac{4}{8} = \frac{1}{2}.
    \end{equation}

    Теперь возьмем в качестве $\varepsilon$ число 6, тогда:
    \begin{equation}
        \probability{X \ge 6} \le \frac{\expectation{X}}{6} = \frac{4}{6} = \frac{2}{3} ,
    \end{equation}
    и перейдем к вероятности дополнительного события в левой части:
    \begin{gather}
        1 - \probability{X < 6} \le \frac{2}{3} , \\
        1 - \frac{2}{3} \le \probability{X < 6} , \\
        \frac{1}{3} \le \probability{X < 6} .
    \end{gather}

    \subsection*{Ответ}
    $\probability{X \ge 8} \le \frac{1}{2}$, $\probability{X < 6} > \frac{1}{3}$.


    \section{Пример 2}

    \subsection*{Условие}
    Случайная величина $X$ является измерением расстояния $L$ сантиметров, не имеющим систематической ошибки, $\expectation{X} = L$, но содержащим случайную погрешность
    измерения, величина которой соответствует среднеквадратическому отклонению $\sigma_X = 5$ сантиметров.

    Оценить сверху вероятность получения плохого (неточного) измерения, с ошибкой не менее 10 сантиметров.

    Во сколько раз уточнится оценка этой вероятности, если дополнительно станет известно, что величина $X$ имеет нормальное распределение $\mathcal{N} ( L, \sigma_X)$?

    \subsection*{Решение}
    Поскольку величина $X$ имеет конечную дисперсию, то для оценки требуемой вероятности воспользуемся вторым неравенством Чебышева:
    \begin{equation}
        \forall \varepsilon > 0 : \probability{ \modulus{X - \expectation{X}} \ge \varepsilon } \le \frac{\variance{X}}{\varepsilon^2} ,
    \end{equation}
    в котором в качестве $\varepsilon$ будем использовать отклонение в 10 сантиметров:
    \begin{gather}
        \probability{ \modulus{X - \expectation{X}} \ge 10 } \le \frac{\variance{X}}{10^2} , \\
        \probability{ \modulus{X - L} \ge 10 } \le \frac{\sigma_X^2}{10^2} , \\
        \probability{ \modulus{X - L} \ge 10 } \le \frac{5^2}{10^2} = \frac{25}{100} = 0.25 \label{2:bound}.
    \end{gather}

    Если величина $X$ имеет нормальное распределение, то вероятность отклонения на 10 сантиметров можно вычислить точно:
    \begin{multline}
        \label{2:precise}
        \probability{\modulus{X - \expectation{X}} \ge 10}
        = \probability{X - \expectation{X} \le - 10} + \probability{ X - \expectation{X} \ge 10} = \\
        %
        = \probability{X - \expectation{X} \le - 10} + 1 - \probability{ X - \expectation{X} < 10} = \\
        %
        = \probability{X \le \expectation{X} - 10} + 1 - \probability{ X < \expectation{X} + 10} = \\
        %
        = \Phi \left ( \frac{\expectation{X} - 10 - \expectation{X}}{\sigma_X} \right ) + 1 - \Phi \left ( \frac{\expectation{X} + 10 - \expectation{X}}{\sigma_X} \right ) = \\
        %
        = \Phi \left ( \frac{- 10}{\sigma_X} \right ) + 1 - \Phi \left ( \frac{10}{\sigma_X} \right )
        = \Phi \left ( \frac{- 10}{\sigma_X} \right ) + \Phi \left ( \frac{- 10}{\sigma_X} \right )
        = 2 \Phi \left ( \frac{- 10}{\sigma_X} \right )
        = 2 \Phi \left ( \frac{- 10}{5} \right ) = \\
        %
        = 2 \Phi \left ( - 2 \right )
        \approx 2 * 0.0228
        = 0.0456
    \end{multline}

    Сравнивая оценку сверху из неравенства Чебышева \eqref{2:bound} и точное значение вероятности \eqref{2:precise}:
    \begin{equation}
        \nu = \frac{0.25}{0.0456} \approx 5.48
    \end{equation}
    убеждаемся, что оценка по неравенству Чебышева является достаточно "грубой"{} и завышает границу вероятности более чем в 5 раз.
    \subsection*{Ответ}
    \begin{enumerate}
        \item $ \probability{ \modulus{X - L} \ge 10 } \le 0.25 $,
        \item в случае нормального распределения оценка вероятности уточняется более чем в 5 раз.
    \end{enumerate}


    \section{Пример 3}
    \subsection*{Условие}
    В последовательности $\sequence{X_n}$ случайные величины $X_n$ имеют нормальное распределение $\mathcal{N} \left ( m, \frac{\sigma^2}{n^2} \right )$.
    Показать, что последовательность величин $\sequence{X_n}$ сходится по вероятности к числу $m$.

    \subsection*{Решение}
    Требуется показать, что $X_n \stackrel{P}{\longrightarrow} m$, то есть:
    \begin{equation}
        \forall \varepsilon > 0: \lim_{n \rightarrow \infty} \probability{ \modulus{ X_n - m } \ge \varepsilon } = 0 .
    \end{equation}
    Вычислим вероятность, стоящую под знаком предела:
    \begin{multline}
        \probability{\modulus{ X_n - m } \ge \varepsilon}
        = \probability{X_n - m \le - \varepsilon} + \probability{X_n - m \ge \varepsilon} = \\
        %
        = \probability{X_n - m \le - \varepsilon} + 1 - \probability{X_n - m < \varepsilon} = \\
        %
        = \probability{X_n \le m - \varepsilon} + 1 - \probability{X_n < m + \varepsilon}
        = \Phi \left ( \frac{ m - \varepsilon - m}{\frac{\sigma}{n}} \right ) + 1 - \Phi \left ( \frac{m + \varepsilon - m}{\frac{\sigma}{n}} \right ) = \\
        %
        = \Phi \left ( n \frac{-\varepsilon}{\sigma} \right ) + 1 - \Phi \left ( n \frac{\varepsilon}{\sigma} \right )
        = \Phi \left ( - n \frac{\varepsilon}{\sigma} \right ) + \Phi \left ( - n \frac{\varepsilon}{\sigma} \right )
        = 2 \Phi \left ( - n \frac{\varepsilon}{\sigma} \right ).
    \end{multline}
    Теперь вычислим предел:
    \begin{equation}
        \lim_{n \rightarrow \infty} \probability{\modulus{ X_n - m } \ge \varepsilon}
        = \lim_{n \rightarrow \infty} 2 \Phi \left ( - n \frac{\varepsilon}{\sigma} \right )
        = 2 \lim_{n \rightarrow \infty} \Phi \left ( - n \frac{\varepsilon}{\sigma} \right )
        = 2 \cdot 0
    \end{equation}
    поскольку
    \begin{equation}
        \lim_{n \rightarrow \infty} - n \frac{\varepsilon}{\sigma} = - \infty .
    \end{equation}

    На рисунках \ref{3:convergence} представлены плотности вероятности двух величин: на левом $X_{n_1}$ и на правом $X_{n_2}$, при $n_1 < n_2$. С ростом $n$ плотность
    вероятности сжимается в окрестности математического ожидания $m$ и вероятности в областях 1 и 2 уменьшаются --- вероятности отклонения
    $\probability{ \modulus{ X_n - m } \ge \varepsilon }$ уменьшаются и стремятся к 0.

    \begin{figure}[h]
        \centering
        \begin{subfigure}{0.4\textwidth}
            \begin{tikzpicture}[scale=3]
                % оси
                \draw [->] ( -0.1, 0 ) -- ( 2, 0 );
                \draw [->] ( 0, -0.1 ) -- ( 0, 2 );

                % плотность
                \draw [domain=0.1:1.9] plot (\x, {exp(-5*(\x-1)*(\x-1))});
                \node [right] at ( 1.5, 0.5 ) {$p_{n_1}(x)$};

                % математическое ожидание
                \draw [dotted] ( 1, 0 ) -- ( 1, 2 ) node [above] at ( 1, 2 ) {$m$};

                % границы
                \draw [dashed] ( 0.7, 0 ) -- ( 0.7, 0.9 ) node [below] at ( 0.7, 0 ) {$m - \varepsilon$};
                \draw [dashed] ( 1.3, 0 ) -- ( 1.3, 0.9 ) node [below] at ( 1.3, 0 ) {$m + \varepsilon$};
            \end{tikzpicture}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.4\textwidth}
            \begin{tikzpicture}[scale=3]
                % оси
                \draw [->] ( -0.1, 0 ) -- ( 2, 0 );
                \draw [->] ( 0, -0.1 ) -- ( 0, 2 );

                % плотность
                \draw [domain=0.1:1.9] plot (\x, {exp(-15*(\x-1)*(\x-1))*1.8});

                % математическое ожидание
                \draw [dotted] ( 1, 0 ) -- ( 1, 2 ) node [above] at ( 1, 2 ) {$m$};
                \node [right] at ( 1.5, 0.5 ) {$p_{n_2}(x)$};

                % границы
                \draw [dashed] ( 0.7, 0 ) -- ( 0.7, 0.9 ) node [below] at ( 0.7, 0 ) {$m - \varepsilon$};
                \draw [dashed] ( 1.3, 0 ) -- ( 1.3, 0.9 ) node [below] at ( 1.3, 0 ) {$m + \varepsilon$};
            \end{tikzpicture}
        \end{subfigure}
        \caption{Сходимость по вероятности.}
        \label{3:convergence}
    \end{figure}


    \section{Задача 18.551}
    \subsection*{Условие}
    Задана последовательность попарно независимых случайных величин $\sequence{X_n}$ ($n=1,2,\dots$):

    \begin{tabular}{|c|c|c|c|}
        \hline
        $x_{ni}$                     & $-n a$            & 0                   & $na$             \\
        \hline
        $\probability{X_n = x_{ni}}$ & $\frac{1}{2 n^2}$ & $1 - \frac{1}{n^2}$ & $\frac{1}{2n^2}$ \\
        \hline
    \end{tabular}

    Выяснить выполняется ли закон больших чисел для последовательности $\sequence{X_n}$.

    \subsection*{Решение}
    Попробуем выяснить выполнение закона больших чисел с помощью теоремы Чебышева: если условие теоремы будет выполнено,
    тогда можно утверждать, что для последовательности закон больших чисел выполняется. Однако, если условие теоремы не будет выполнено, то
    \textbf{нельзя утверждать}, что закон больших чисел не выполняется для последовательности $\sequence{X_n}$, поскольку условие теоремы является только достаточным условием.

    Итак, теорема Чебышева требует, чтобы в последовательности $\sequence{X_n}$ случайные величины были попарно независимы --- эта часть условия
    теоремы выполнена в соответствии с условиями задачи, и ещё необходимо убедиться, что сумма дисперсий величин последовательности растёт не очень быстро,
    точнее не быстрее чем $n^2$:
    \begin{equation}
        \lim_{n \rightarrow \infty} \frac{1}{n^2} \sum_{k=1}^n \variance{X_k} \stackrel{?}{=} 0 .
    \end{equation}

    Вычислим дисперсию отдельных случайных величин $X_k$:
    \begin{align}
        \variance{X_k} & = \expectation{X_k^2} - \left ( \expectation{X_k} \right )^2 , \\
        %
        \expectation{X_k^2}
        & = \sum_{i=1}^3 x_{ki}^2 \probability{X_k = x_{ki}}
        = \left ( -na \right )^2 \cdot \frac{1}{2 n^2} + 0 \cdot \left ( 1 - \frac{1}{n^2} \right ) + \left ( na \right )^2 \cdot \frac{1}{2 n^2} = \notag \\
        & = n^2 a^2 \cdot \frac{1}{2 n^2} + n^2 a^2 \cdot \frac{1}{2 n^2}
        = \frac{a^2}{2} + \frac{a^2}{2} = a^2 , \\
        %
        \expectation{X_k}
        & = \sum_{i=1}^3 x_{ki} \probability{X_k = x_{ki}}
        = \left ( -na \right ) \cdot \frac{1}{2 n^2} + 0 \cdot \left ( 1 - \frac{1}{n^2} \right ) + \left ( na \right ) \cdot \frac{1}{2 n^2} = \notag \\
        & = - \frac{a}{2 n} + \frac{a}{2 n} = 0,\\
        %
        \variance{X_k} & = a^2 - 0^2 = a^2 .
    \end{align}

    Таким образом,
    \begin{equation}
        \lim_{n \rightarrow \infty} \frac{1}{n^2} \sum_{k=1}^n \variance{X_k}
        = \lim_{n \rightarrow \infty} \frac{1}{n^2} \sum_{k=1}^n a^2
        = \lim_{n \rightarrow \infty} \frac{1}{n^2} n a^2
        = a^2 \lim_{n \rightarrow \infty} \frac{1}{n} = a^2 \cdot 0 = 0,
    \end{equation}
    и условия теоремы Чебышева выполнены, поэтому для последовательности $\sequence{X_n}$ выполняется закон больших чисел.

    \subsection*{Ответ}
    Выполняется.


    \section{Пример 4}
    \subsection*{Условие}
    Младший научный сотрудник Иван Петров работает в лаборатории высокоточных измерений Института Метрологии и Ядерного Синтеза (ИМЯС).
    Вчера в лабораторию поступил внушительных размеров стержень из неизвестного материала, обладавший мягким, приятным и едва уловимым свечением,
    с припиской руководства о необходимости как можно скорее определить его длину, да так, чтобы с вероятностью $P_\text{д} = 0.95$ ошибка измерения составляла менее
    $\Delta = 2$ сантиметров. Задача эта и в половину не была бы столь сложна, если бы после долгих поисков не выяснилось, что в лаборатории, где можно измерять
    микрометрические длины с точностью до нанометров, нет на одной захудалой рулетки.

    Сложившаяся ситуация требовала ответов на два извечных русских вопроса.
    \begin{itemize}
        \item Кто виноват?
        \item [] и
        \item Что делать?
    \end{itemize}

    \subsection*{Решение}
    Иван Петров уже давно заметил, что решение любой проблемы в ИМЯСе начиналось не с самой проблемы, как этого следовало бы ожидать, а с поиска виноватых, и этими
    виноватыми, как правило, оказывались простые, рядовые исполнители, и потому Иван Петров, хотя и был еще очень молодым, но уже утратившим некоторые иллюзии человеком,
    без особого труда ответил себе на первый вопрос, и тот сразу перестал его занимать.

    Второй вопрос оказался для Ивана Петрова гораздно более увлекательным. Когда-то в молодости, еще не так давно минувшей, Иван Петров учился на МехМате МГУ, и хотя был
    отчислен за неуспеваемость, но годы, проведенные в СУНЦ им. Колмогорова, и первые два курса не прошли для него даром, и очень скоро светлая мысль, как обычно, посетила
    его юное, но невероятно талантливое сознание.

    Иван Петров решительно встал и, схватив стержень, выскочил с ним в коридор.

    Наткнувшись в коридоре на Юрия Николаевича, Петров, не здороваясь, ошарашил его вопросом о
    длине стержня, на что Ю. Н., не моргнув глазом, коротко ответил: "1 метр 76 сантиметров и еще 3 миллиметра". Петров старательно записал длину в блокнот и
    ринулся дальше по коридору, оставив растерянного коллегу рассматривать свою спину в белом халате.

    Петров мыслил так: он будет спрашивать у всех кого встретит длину стержня и записывать длины величинами $X_n$, пусть они неправильны и неточны и содержат ошибки, он сумеет
    их обработать, а крайние измерения он и вовсе выбросит, чтобы не портили дела.

    Петров прекрасно понимал, что
    \begin{equation}
        \forall \varepsilon > 0: \lim_{n \rightarrow \infty} \probability{ \modulus{ \frac{1}{n} \sum_{k=1}^n X_k - \frac{1}{n} \sum_{k=1}^n \expectation{X_k} } \ge \varepsilon } = 0
    \end{equation}
    и это внушало надежду. Оставалось лишь набрать достаточное количество $n$. Только бы не было смещения, подумалось было Петрову, но он решительно отбросил эту мысль
    как негативную --- в обществе прогрессировало безоглядно-позитивное мышление.

    В следующую половину дня Петрова видели в разных частях Института, застревающим в лифте, сбивающим людей на лестнице, и в группе лиц, усиленно потиравших наморщенные лбы
    и хватавших себя за подбородок. Иногда, Петрова просили встать поближе к стрежню, спрашивали его рост, некоторым казалось, что так измерение выйдет точнее, хотя это было
    не совсем честно. Петров наотрез отказывался от предлагаемых линеек, дотошно и с пристрастием требовал ответить на вопрос о длине --- идея захватила всё его существо.

    Однако изрядно набегавшись, Петров решил наконец пообедать и за обедом стал думать о том, сколько же ему еще придется так бегать. Предположим, думал Петров, что все $\expectation{X_n} = L$, где $L$ --- пресловутая длина стержня, тогда:
    \begin{gather}
        S_n = \frac{1}{n} \sum_{k=1}^n X_k \stackrel{P}{\longrightarrow} \frac{1}{n} \sum_{k=1}^n \expectation{X_k} = \frac{1}{n} \sum_{k=1}^n L = \frac{1}{n} \cdot n \cdot L = L , \\
        S_n = \frac{1}{n} \sum_{k=1}^n X_k \stackrel{P}{\longrightarrow} L ,
    \end{gather}
    но сходимость --- сходимостью, а точность --- точностью, и теперь настало время проявить свои незаурядные математические способности и заняться вплотную числами 0.95 и 2.

    Петров взял салфетку и стал быстро набрасывать мелким почерком. Вообще, величины $S_n$ ничем не хуже любых других, размышлял Петров, поэтому для них скорее всего выполняется
    неравенство Чебышева, и коль скоро:
    \begin{equation}
        \expectation{S_n}
        = \expectation{\frac{1}{n} \sum_{k=1}^n X_k}
        = \frac{1}{n} \expectation{\sum_{k=1}^n X_k}
        = \frac{1}{n} \sum_{k=1}^n \expectation{X_k}
        = \frac{1}{n} \sum_{k=1}^n L
        = \frac{1}{n} \cdot n \cdot L
        = L ,
    \end{equation}
    то и по неравенству выходит, что
    \begin{equation}
        \probability{\modulus{S_n - L} \ge \Delta} \le \frac{\variance{S_n}}{\Delta^2} .
    \end{equation}
    Это почти то, что нужно --- у нас ведь требование:
    \begin{gather}
        \probability{\modulus{S_n - L} < \Delta} \ge P_\text{д} , \\
        1 - \probability{\modulus{S_n - L} \ge \Delta} \ge P_\text{д} , \\
        1 - P_\text{д} \ge \probability{\modulus{S_n - L} \ge \Delta} , \\
        \probability{\modulus{S_n - L} \ge \Delta} \le 1 - P_\text{д} ,
    \end{gather}
    и если сделать так, чтобы выполнялось двойное неравество:
    \begin{equation}
        \probability{\modulus{S_n - L} \ge \Delta} \le \frac{\variance{S_n}}{\Delta^2} \le 1 - P_\text{д} ,
    \end{equation}
    то и требование задачи будет выполнено --- значит нужно подобрать $n$ так, чтобы дисперсия удовлетворяла неравенству:
    \begin{gather}
        \frac{\variance{S_n}}{\Delta^2} \le 1 - P_\text{д} , \\
        \variance{S_n} \le \Delta^2 \cdot \left ( 1 - P_\text{д} \right ) \label{5:variance}.
    \end{gather}
    При этом дисперсия в левой части:
    \begin{equation}
        \variance{S_n}
        = \variance{\frac{1}{n} \sum_{k=1}^n X_k}
        = \frac{1}{n^2} \variance{\sum_{k=1}^n X_k},
    \end{equation}
    пусть все величины попарно независимы, это без особой натяжки можно считать выполненным, тогда:
    \begin{equation}
        \variance{S_n}
        = \frac{1}{n^2} \sum_{k=1}^n \variance{X_k}
        = \frac{1}{n^2} \sum_{k=1}^n \sigma_X^2
        = \frac{1}{n^2} \cdot n \cdot \sigma_X^2
        = \frac{\sigma_X^2}{n} ,
    \end{equation}
    где $\sigma_X$ --- среднеквадратическое отклонение ошибки определения длины респондентами. Так, хорошо, думал Петров, теперь берем неравенство \eqref{5:variance}:
    \begin{gather}
        \frac{\sigma_X^2}{n} \le \Delta^2 \cdot \left ( 1 - P_\text{д} \right ) , \\
        \frac{\sigma_X^2}{\Delta^2 \cdot \left ( 1 - P_\text{д} \right )} \le n , \\
        n \ge \frac{\sigma_X^2}{\Delta^2 \cdot \left ( 1 - P_\text{д} \right )} = \frac{\sigma_X^2}{2^2 \cdot \left ( 1 - 0.95 \right )} = \frac{\sigma_X^2}{4 \cdot 0.05} = 5 \sigma_X^2,
    \end{gather}
    Нда, вот это уже не очень хорошо. Если ошибка определения длины имеет среднеквадратическое отклонение $\sigma_X = 20$ сантиметров, то уже:
    \begin{equation}
        n \ge 5 \sigma_X^2 = 5 \cdot 20^2 = 2000.
    \end{equation}
    Нет, 2000 человек я не набегаю, размышлял Петров. Придется считать измерения более точными, скажем $\sigma_X = 5$ сантиметров, тогда:
    \begin{equation}
        n \ge 5 \sigma_X^2 = 5 \cdot 5^2 = 125.
    \end{equation}
    Совсем другое дело --- поздравил себя Петров.

    Уже начинало темнеть, когда Петров, опросив всех знакомых и незнакомых, заглянув в бухгалтерию и дождавшись уборщицу Марию Степановну, записал в свой блокнот результат
    обработки всех измерений --- 1 метр 76 сантиметров и 1 миллиметр. Подумать только, мелькнуло в голове Ивана Петрова, но он не придал этому значения и счастливый, необычайно
    довольный собой, уже ехал домой в трамвае в направлении заката.

    А на следующее утро Петрову сообщили, что ничего измерять не нужно, и что обошлись без этого стержня, но Ивану Петрову уже было всё равно --- он измерял стержень
    своей рулеткой, которую привез из дома.

    \subsection*{Ответ}
    Даже из многих плохих измерений можно сделать одно хорошее, если, например, измерения не имеют смещения, независимы и обладают конечной дисперсией.


    \section{Домашнее задание}
    Типовой расчет 32

    Задачи 543, 544, 546, 550, 552.
\end{document}